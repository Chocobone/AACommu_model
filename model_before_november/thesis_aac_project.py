import torch
import pandas as pd
import re
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    GPT2LMHeadModel,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
import warnings
import json
import zipfile # <-- zipfile ëª¨ë“ˆ ì¶”ê°€
import os

warnings.filterwarnings("ignore")
print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!")

# --- 1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„ ì • ---
MODEL_NAME = "skt/kogpt2-base-v2"
# ğŸš¨ ìˆ˜ì •: ZIP íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œë¡œ ë³€ê²½
TRAINING_DIR = "/local_datasets/AACommu/Training/02.ë¼ë²¨ë§ë°ì´í„°/"
VALIDATION_DIR = "/local_datasets/AACommu/Validation/02.ë¼ë²¨ë§ë°ì´í„°/" # ê²€ì¦ ë°ì´í„° ê²½ë¡œë„ ìœ ì‚¬í•  ê²ƒìœ¼ë¡œ ê°€ì •
FINAL_MODEL_PATH = "/data/yho7374/repos/AACommu_model/thesis_aac_model"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
special_tokens = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>', 'sep_token': '<sep>'}
tokenizer.add_special_tokens(special_tokens)

model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
model.resize_token_embeddings(len(tokenizer))

print("Chapter 1: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ!")

# --- 2. ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (ZIP íŒŒì¼ ì²˜ë¦¬ë¡œ ë³€ê²½) ---
def load_data_from_zip_dir(data_dir):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  ZIP íŒŒì¼ì„ ì—´ì–´, ê·¸ ì•ˆì— ìˆëŠ” ëª¨ë“  JSON íŒŒì¼ì„ ì½ì–´ í†µí•©í•©ë‹ˆë‹¤.
    """
    all_data = []
    
    try:
        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ìˆœíšŒ
        for filename in os.listdir(data_dir):
            if filename.endswith('.zip'):
                zip_path = os.path.join(data_dir, filename)
                print(f"   ZIP íŒŒì¼ '{filename}'ì„ ì—½ë‹ˆë‹¤...")
                
                with zipfile.ZipFile(zip_path, 'r') as zf:
                    # ZIP íŒŒì¼ ë‚´ë¶€ì˜ ëª¨ë“  ë©¤ë²„ë¥¼ ìˆœíšŒ
                    for member_name in zf.namelist():
                        # .json íŒŒì¼ë§Œ ì²˜ë¦¬
                        if member_name.endswith('.json'):
                            # zf.open()ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ê°ì²´ë¥¼ ì–»ê³  ë°”ë¡œ ì½ìŠµë‹ˆë‹¤.
                            with zf.open(member_name) as f:
                                data = f.read().decode('utf-8-sig')
                                
                                # JSON ë¡œë”© ë° í†µí•©
                                json_data = json.loads(data)
                                
                                if isinstance(json_data, list):
                                    all_data.extend(json_data)
                                else:
                                    all_data.append(json_data)

                                print(f"      [LOAD] {member_name} ë¡œë“œ ì™„ë£Œ.")
        
        return all_data
    
    except FileNotFoundError:
        print(f"Error: '{data_dir}' ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()
    except Exception as e:
        print(f"Error processing data in {data_dir}: {e}")
        exit()

# --- 2. í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ë° í†µí•© ---
print(f"Chapter 2: '{TRAINING_DIR}' ë° '{VALIDATION_DIR}'ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹œì‘!")

# í•™ìŠµ ë°ì´í„° ë¡œë“œ
training_data = load_data_from_zip_dir(TRAINING_DIR)
print(f"ì´ {len(training_data)}ê°œì˜ í•™ìŠµ ë°ì´í„° ë ˆì½”ë“œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
train_df = pd.DataFrame(training_data)

# ê²€ì¦ ë°ì´í„° ë¡œë“œ
validation_data = load_data_from_zip_dir(VALIDATION_DIR)
print(f"ì´ {len(validation_data)}ê°œì˜ ê²€ì¦ ë°ì´í„° ë ˆì½”ë“œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
eval_df = pd.DataFrame(validation_data)

# ğŸš¨ ì¤‘ìš”: ë¼ë²¨ë§ ë°ì´í„°ì˜ í•„ë“œ í™•ì¸ ë° ë§ì¶¤ (ì´í•˜ ë™ì¼)
# ... (ì´í•˜ ì½”ë“œëŠ” ì›ë³¸ê³¼ ë™ì¼)

if 'input' not in train_df.columns or 'output' not in train_df.columns:
    print("\nâš ï¸ ê²½ê³ : ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„ì— 'input' ë˜ëŠ” 'output' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("      ì›ë³¸ JSON íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì—¬ 'preprocess_function' ë° 'df' ìƒì„± ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    print(f"      í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼: {train_df.columns.tolist()}")
    
raw_datasets = DatasetDict({
    'train': Dataset.from_pandas(train_df),
    'eval': Dataset.from_pandas(eval_df)
})

# --- 3. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¶„ë¦¬ ---

def preprocess_function(examples):
    # 'input'ê³¼ 'output' í•„ë“œë¥¼ ì—°ê²°í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±
    full_texts = [inp + out for inp, out in zip(examples['input'], examples['output'])]
    return tokenizer(full_texts, padding="max_length", truncation=True, max_length=128)

tokenized_datasets = raw_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=raw_datasets['train'].column_names,
)
print("Chapter 3: ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¶„ë¦¬ ì™„ë£Œ!")

# --- 4. ëª¨ë¸ íŒŒì¸íŠœë‹ (ì´í•˜ ë™ì¼) ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Using device: {device}")

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./thesis_aac_model_checkpoints",
    num_train_epochs=10, 
    per_device_train_batch_size=8, 
    learning_rate=5e-5,
    weight_decay=0.01,
    eval_strategy="epoch", 
    save_strategy="epoch",
    load_best_model_at_end=True, 
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    logging_dir='./logs',
    logging_steps=100, 
    save_total_limit=2, 
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["eval"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

print("Chapter 4: ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()
print("ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ!")

trainer.save_model(FINAL_MODEL_PATH)
tokenizer.save_pretrained(FINAL_MODEL_PATH)
print(f"ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì´ '{FINAL_MODEL_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 5. ëŒ€í™”í˜• ì¶”ë¡  ì‹œìŠ¤í…œ ì‹¤í–‰ (í•µì‹¬ ê¸°ëŠ¥) ---
print("\n[ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë”© ì¤‘...]")
inference_tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_PATH)
inference_model = GPT2LMHeadModel.from_pretrained(FINAL_MODEL_PATH)
inference_model.to(device)
inference_model.eval()

# 'ë¬´í˜•ë¬¸í™”ì¬' ê°™ì€ ë¬¸ë§¥ê³¼ ì „í˜€ ìƒê´€ì—†ëŠ” ë‹¨ì–´ ìƒì„±ì„ ì–µì œí•˜ê¸° ìœ„í•œ ID ë¦¬ìŠ¤íŠ¸
bad_words = ["ë¬¸í™”ì¬", "ìœ í˜•", "ë¬´í˜•", "êµ­ë³´", "ë³´ë¬¼", "ì•„íŒŒìš”", "ë³‘ì›"] 
bad_words_ids = [inference_tokenizer.encode(bad_word, add_special_tokens=False) for bad_word in bad_words]
bad_words_ids = [item for sublist in bad_words_ids for item in sublist]


def generate_next_chunk(context, input_text, current_sentence):
    prompt = f"{inference_tokenizer.bos_token}{context}{inference_tokenizer.sep_token}{input_text}{inference_tokenizer.sep_token}{current_sentence}"
    inputs = inference_tokenizer(prompt, return_tensors="pt").to(device)
    input_ids_len = len(inputs.input_ids[0])

    generation_params = {
        "max_length": input_ids_len + 30,
        "num_beams": 5,
        "repetition_penalty": 2.5, 
        "early_stopping": True,
        "eos_token_id": inference_tokenizer.eos_token_id,
        "pad_token_id": inference_tokenizer.pad_token_id,
        "no_repeat_ngram_size": 2, 
    }
    
    if context == "ì¹´í˜":
        generation_params["bad_words_ids"] = bad_words_ids

    
    outputs = inference_model.generate(inputs.input_ids, **generation_params)

    generated_ids = outputs[0][input_ids_len:]
    generated_text = inference_tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    chunks = generated_text.split(inference_tokenizer.sep_token)
    if chunks and chunks[0].strip():
        next_chunk_options = chunks[0].strip().split('/')
        return next_chunk_options
    else:
        return []

def run_interactive_inference():
    print("\n--- Chapter 5: AAC ë…¼ë¬¸ ìˆ˜ì¤€ ëŒ€í™”í˜• ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ì•ˆì •í™”) ---")
    
    test_context = "ì¹´í˜"
    test_input_text = "ì£¼ë¬¸ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    print(f"ì…ë ¥ ìƒí™©: {test_context}")
    print(f"ì…ë ¥ ë°œí™”: {test_input_text}\n")
    
    selected_chunks = []
    
    while True:
        current_sentence = " ".join(selected_chunks)
        if current_sentence:
            current_sentence += " "
            
        next_options = generate_next_chunk(test_context, test_input_text, current_sentence)

        if not next_options or len(selected_chunks) > 7:
            if not next_options:
                print("[ëª¨ë¸] ì¶”ì²œí•  ë‚´ìš©ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        if len(next_options) == 1 and next_options[0] in ["ì£¼ì„¸ìš”", "ì…ë‹ˆë‹¤", "ê°ˆê²Œìš”", "ê´œì°®ì•„ìš”", "ì—†ì–´ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì•„íŒŒìš”", "ì´ìš”", "í• ê²Œìš”", "ì•Œê² ìŠµë‹ˆë‹¤", "ë‚´ë¦´ê²Œìš”", "ì„¸ì›Œì£¼ì„¸ìš”"]:
            print(f"[ëª¨ë¸] ë¬¸ì¥ ì¢…ê²° í† í° '{next_options[0]}'ì´(ê°€) ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            selected_chunks.append(next_options[0])
            break
            
        print("ë‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:")
        for i, option in enumerate(next_options):
            print(f"{i+1}: {option}")
        
        direct_input_option_num = len(next_options) + 1
        print(f"{direct_input_option_num}: (ì§ì ‘ ì…ë ¥)")
        
        try:
            choice_idx = int(input(f"ì„ íƒ (1-{direct_input_option_num}): ")) - 1
            
            if 0 <= choice_idx < len(next_options):
                chosen_chunk = next_options[choice_idx]
                selected_chunks.append(chosen_chunk)
                print(f"\n>> í˜„ì¬ ë¬¸ì¥: {' '.join(selected_chunks)}\n")
            
            elif choice_idx == len(next_options):
                custom_chunk = input("ì§ì ‘ ì…ë ¥í•  í…ìŠ¤íŠ¸: ")
                selected_chunks.append(custom_chunk)
                print(f"\n>> í˜„ì¬ ë¬¸ì¥: {' '.join(selected_chunks)}\n")
            
            else:
                print("ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.\n")
        
        except (ValueError, IndexError):
            print("ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

    final_sentence = " ".join(selected_chunks)
    print("\n--- ìµœì¢… ì™„ì„±ëœ ë¬¸ì¥ ---")
    print(f"'{final_sentence}'")
    print("--------------------")

if __name__ == "__main__":
    run_interactive_inference()