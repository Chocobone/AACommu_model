{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457261c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "DATA_PATH_DEFAULT = os.environ.get(\"CAFE_CHUNKS_PATH\", \"/content/cafe_chunks.jsonl\")\n",
    "TOPK_DEFAULT = 5\n",
    "\n",
    "# ---------- ë¬¸ìì—´ ì „ì²˜ë¦¬ ----------\n",
    "def normalize(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = s.strip()\n",
    "    #s = re.sub(r\"[^0-9A-Za-z\\u3131-\\u318E\\uAC00-\\uD7A3\\s\\.\\,\\?\\!\\-\\~Â·]+\", \" \", s)\n",
    "    #s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^0-9A-Za-z\\u3131-\\u318E\\uAC00-\\uD7A3\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def bm25_tokenize(s: str):\n",
    "    s = normalize(s).lower()\n",
    "    return s.split() if s else []\n",
    "\n",
    "def split_chunk_to_tokens(ch: str):\n",
    "    ch = normalize(ch)\n",
    "    return ch.split() if ch else []\n",
    "\n",
    "def split_list_chunks_to_tokens(chunks):\n",
    "    toks = []\n",
    "    for ch in chunks or []:\n",
    "        toks.extend(split_chunk_to_tokens(str(ch)))\n",
    "    return toks\n",
    "\n",
    "# ---------- ë°ì´í„° ë¡œë”© ----------\n",
    "def load_step_examples(path: str):\n",
    "    examples = []\n",
    "    if not os.path.exists(path):\n",
    "        return examples\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                ex=json.loads(line)\n",
    "                if \"selected_chunks\" in ex and \"target_chunk\" in ex:\n",
    "                    examples.append(ex)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return examples\n",
    "\n",
    "# ---------- n-gram ìœ ì‚¬ë„ (STT fuzzy matchìš©) ----------\n",
    "def char_ngrams(s: str, n=3):\n",
    "    s = normalize(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    if len(s) < n:\n",
    "        return {s} if s else set()\n",
    "    return {s[i:i+n] for i in range(len(s)-n+1)}\n",
    "\n",
    "def jaccard(a: set, b: set):\n",
    "    if not a and not b: return 1.0\n",
    "    if not a or not b: return 0.0\n",
    "    inter = len(a & b)\n",
    "    union = len(a | b)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "# ---------- í†µê³„ ë§Œë“¤ê¸° ----------\n",
    "def build_stats_from_examples(examples):\n",
    "    vocab = Counter()\n",
    "    trans = defaultdict(Counter)      # prev -> next\n",
    "    trans2 = defaultdict(Counter)     # (prev2, prev1) -> next\n",
    "    candidates = set()\n",
    "    # STTë³„ ì‹œì‘ ë¶„í¬: user_asr_norm -> Counter(next_token)\n",
    "    start_counts_by_asr = defaultdict(Counter)\n",
    "\n",
    "    # ëª¨ë“  user_asr ëª¨ìŒ (ìœ ì‚¬ë„ íƒìƒ‰ìš©)\n",
    "    asr_set = set()\n",
    "\n",
    "    for ex in examples:\n",
    "        user_asr = ex.get(\"user_asr\",\"\")\n",
    "        user_asr_norm = normalize(user_asr)\n",
    "        asr_set.add(user_asr_norm)\n",
    "\n",
    "        selected = split_list_chunks_to_tokens(ex.get(\"selected_chunks\", []))\n",
    "        target_tokens = split_chunk_to_tokens(ex.get(\"target_chunk\",\"\"))\n",
    "\n",
    "        # ì „ì—­ vocab/í›„ë³´ëŠ” target ê¸°ì¤€\n",
    "        for t in target_tokens:\n",
    "            vocab[t] += 1\n",
    "            candidates.add(t)\n",
    "\n",
    "        # ì‹œì‘ ë¶„í¬: selectedê°€ ë¹ˆ ê²½ìš°, targetì˜ ì²« í† í°ì„ ì‹œì‘ìœ¼ë¡œ ê°„ì£¼\n",
    "        if not selected and target_tokens:\n",
    "            start_counts_by_asr[user_asr_norm][target_tokens[0]] += 1\n",
    "\n",
    "        # ì „ì´: selected ë’¤ì— targetì´ ìˆœì°¨ì ìœ¼ë¡œ ì´ì–´ì§„ë‹¤ê³  ê°€ì •\n",
    "        ctx = list(selected)\n",
    "        for tok in target_tokens:\n",
    "            if len(ctx) >= 1:\n",
    "                trans[ctx[-1]][tok] += 1\n",
    "            if len(ctx) >= 2:\n",
    "                trans2[(ctx[-2], ctx[-1])][tok] += 1\n",
    "            ctx.append(tok)\n",
    "\n",
    "    return vocab, trans, trans2, candidates, start_counts_by_asr, asr_set\n",
    "\n",
    "# ---------- ì ìˆ˜ ê³„ì‚° ----------\n",
    "def transition_score(candidate, trans, trans2, ctx_tail):\n",
    "    eps = 1e-9\n",
    "    s = 0.0\n",
    "    if len(ctx_tail) >= 2:\n",
    "        prev2 = (ctx_tail[-2], ctx_tail[-1])\n",
    "        tbl2 = trans2.get(prev2, {})\n",
    "        tot2 = sum(tbl2.values())\n",
    "        if tot2 > 0:\n",
    "            s += 0.7 * ((tbl2.get(candidate,0)+eps)/(tot2+eps))\n",
    "    if len(ctx_tail) >= 1:\n",
    "        prev1 = ctx_tail[-1]\n",
    "        tbl1 = trans.get(prev1, {})\n",
    "        tot1 = sum(tbl1.values())\n",
    "        if tot1 > 0:\n",
    "            s += 0.3 * ((tbl1.get(candidate,0)+eps)/(tot1+eps))\n",
    "    return s\n",
    "\n",
    "def freq_prior_score(candidate, vocab: Counter):\n",
    "    tot = sum(vocab.values()) or 1.0\n",
    "    return vocab.get(candidate, 0) / tot\n",
    "\n",
    "def lexical_match_score(candidate, user_asr, ctx_tail):\n",
    "    q = set(bm25_tokenize(user_asr))\n",
    "    c = set(bm25_tokenize(candidate))\n",
    "    if not c: return 0.0\n",
    "    overlap = len(q & c)\n",
    "    bonus = 0.0\n",
    "    if ctx_tail:\n",
    "        last = ctx_tail[-1]\n",
    "        if last and candidate and candidate[0] == last[0]:\n",
    "            bonus += 0.03\n",
    "    return overlap + bonus\n",
    "\n",
    "# ---------- í›„ë³´ ìˆ˜ì§‘ ----------\n",
    "def gather_candidates(candidates, trans, trans2, context_tokens, vocab, start_prior=None, widen=True):\n",
    "    cand = set()\n",
    "    # ì „ì´ ê¸°ë°˜\n",
    "    if context_tokens:\n",
    "        cand |= set(trans.get(context_tokens[-1], {}).keys())\n",
    "    if len(context_tokens) >= 2:\n",
    "        cand |= set(trans2.get((context_tokens[-2], context_tokens[-1]), {}).keys())\n",
    "    # ì‹œì‘ ë¶„í¬ ê¸°ë°˜(ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ê³  start_priorê°€ ìˆìœ¼ë©´ ìš°ì„  ì¶”ê°€)\n",
    "    if not context_tokens and start_prior:\n",
    "        cand |= set(start_prior.keys())\n",
    "    # widen\n",
    "    if widen or not cand:\n",
    "        cand |= {w for w, _ in vocab.most_common(300)}\n",
    "    cand |= set(candidates)\n",
    "    return [c for c in cand if c]\n",
    "\n",
    "# ---------- STT ì¡°ê±´ë¶€ ì‹œì‘ ì ìˆ˜(ë³´ê°„) ----------\n",
    "def asr_start_score(candidate, user_asr_input, start_counts_by_asr, asr_set):\n",
    "    \"\"\"ì…ë ¥ STTì™€ ê°€ì¥ ìœ ì‚¬í•œ STTì˜ ì‹œì‘ ë¶„í¬ë¥¼ ì°¾ì•„ ì ìˆ˜ë¡œ ë°˜í™˜.\"\"\"\n",
    "    user_norm = normalize(user_asr_input)\n",
    "    cand_score = 0.0\n",
    "\n",
    "    # 1) ì •í™•íˆ ê°™ì€ STTê°€ ìˆìœ¼ë©´ ê·¸ ë¶„í¬ ì‚¬ìš©\n",
    "    if user_norm in start_counts_by_asr:\n",
    "        sc = start_counts_by_asr[user_norm]\n",
    "        tot = sum(sc.values()) or 1.0\n",
    "        cand_score = sc.get(candidate, 0) / tot\n",
    "\n",
    "    # 2) ìœ ì‚¬ STT fallback (3-gram Jaccard)\n",
    "    if cand_score == 0.0 and asr_set:\n",
    "        a_ngrams = char_ngrams(user_norm, n=3)\n",
    "        best_sim, best_asr = 0.0, None\n",
    "        for a in asr_set:\n",
    "            sim = jaccard(a_ngrams, char_ngrams(a, n=3))\n",
    "            if sim > best_sim:\n",
    "                best_sim, best_asr = sim, a\n",
    "        if best_asr is not None and best_sim >= 0.20:  # ì„ê³„ê°’ ì¡°ì •\n",
    "            sc = start_counts_by_asr.get(best_asr, {})\n",
    "            tot = sum(sc.values()) or 1.0\n",
    "            cand_score = sc.get(candidate, 0) / tot\n",
    "        # ë„ˆë¬´ ë‚®ìœ¼ë©´ ì‚¬ì‹¤ìƒ 0ìœ¼ë¡œ ë‘”ë‹¤\n",
    "\n",
    "    return cand_score\n",
    "\n",
    "# ---------- ë­í‚¹ ----------\n",
    "def rank_candidates(cand_pool, user_asr, context_tokens, vocab, trans, trans2,\n",
    "                    start_counts_by_asr, asr_set, k=5):\n",
    "    counts_in_ctx = Counter(context_tokens)\n",
    "    out = []\n",
    "    for c in cand_pool:\n",
    "        if not c: continue\n",
    "\n",
    "        # ë³´ê°„ ê°€ì¤‘ì¹˜: ì‹œì‘ì´ë©´ STT ì‹œì‘ë¶„í¬ ë¹„ì¤‘ì„ í¬ê²Œ\n",
    "        if not context_tokens:\n",
    "            # ì‹œì‘ ìƒíƒœì¼ ë•Œ: asr_start 0.6 + prior 0.25 + lexical 0.05 + tiny transition(0.1)\n",
    "            s_asr = asr_start_score(c, user_asr, start_counts_by_asr, asr_set)\n",
    "            s = (0.60 * s_asr\n",
    "                 + 0.10 * transition_score(c, trans, trans2, context_tokens)\n",
    "                 + 0.05 * lexical_match_score(c, user_asr, context_tokens)\n",
    "                 + 0.25 * freq_prior_score(c, vocab))\n",
    "        else:\n",
    "            # ì§„í–‰ ì¤‘: transition 0.65 + prior 0.25 + lexical 0.10\n",
    "            s = (0.65 * transition_score(c, trans, trans2, context_tokens)\n",
    "                 + 0.10 * lexical_match_score(c, user_asr, context_tokens)\n",
    "                 + 0.25 * freq_prior_score(c, vocab))\n",
    "\n",
    "        # ë°˜ë³µ íŒ¨ë„í‹°\n",
    "        s *= (1.0 / (1.0 + counts_in_ctx.get(c, 0)))\n",
    "        out.append((c, s))\n",
    "\n",
    "    out.sort(key=lambda x: (-x[1], len(x[0])))\n",
    "    dedup, seen = [], set()\n",
    "    for c, s in out:\n",
    "        if c in seen: continue\n",
    "        seen.add(c); dedup.append((c, s))\n",
    "        if len(dedup) >= k: break\n",
    "    return dedup\n",
    "\n",
    "# ---------- ë©”ì¸ ----------\n",
    "class StepwiseChunkPicker:\n",
    "    def __init__(self, data_path: str = DATA_PATH_DEFAULT):\n",
    "        self.examples = load_step_examples(data_path)\n",
    "        if not self.examples:\n",
    "            raise RuntimeError(f\"ë°ì´í„°ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤: {data_path}\")\n",
    "        (self.vocab, self.trans, self.trans2, self.candidates,\n",
    "         self.start_counts_by_asr, self.asr_set) = build_stats_from_examples(self.examples)\n",
    "\n",
    "    def suggest(self, user_asr: str, selected_chunks_tokens, topk=TOPK_DEFAULT):\n",
    "        start_prior = None\n",
    "        if not selected_chunks_tokens:\n",
    "            # í•´ë‹¹ STT(ë˜ëŠ” ê·¼ì ‘ STT)ì˜ ì‹œì‘ ë¶„í¬ë¥¼ cand pool ìƒì„±ì— ë°˜ì˜\n",
    "            # cand pool ë‚´ë¶€ì—ì„œ ê°€ì¤‘ì¹˜ëŠ” rank_candidatesì—ì„œ ë‹¤ì‹œ ì ìš©\n",
    "            user_norm = normalize(user_asr)\n",
    "            start_prior = self.start_counts_by_asr.get(user_norm, None)\n",
    "            if start_prior is None:\n",
    "                # ê·¼ì ‘ STT í•˜ë‚˜ ê³¨ë¼ì„œ start_priorë¡œ ì‚¬ìš©\n",
    "                best_asr = None\n",
    "                best_sim = 0.0\n",
    "                a_ngrams = char_ngrams(user_norm, n=3)\n",
    "                for a in self.asr_set:\n",
    "                    sim = jaccard(a_ngrams, char_ngrams(a, n=3))\n",
    "                    if sim > best_sim:\n",
    "                        best_sim, best_asr = sim, a\n",
    "                if best_asr and best_sim >= 0.25:\n",
    "                    start_prior = self.start_counts_by_asr.get(best_asr, None)\n",
    "\n",
    "        cand_pool = gather_candidates(self.candidates, self.trans, self.trans2,\n",
    "                                      selected_chunks_tokens, self.vocab,\n",
    "                                      start_prior=start_prior, widen=True)\n",
    "        ranked = rank_candidates(cand_pool, user_asr, selected_chunks_tokens,\n",
    "                                 self.vocab, self.trans, self.trans2,\n",
    "                                 self.start_counts_by_asr, self.asr_set, k=topk)\n",
    "        return ranked\n",
    "\n",
    "    def simulate_app_input(self, topk=TOPK_DEFAULT):\n",
    "        user_asr = input(\"ğŸ‘‚ ì ì›(STT) ë§ ì…ë ¥: \").strip()\n",
    "        context_tokens = []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            ranked = self.suggest(user_asr, context_tokens, topk=topk)\n",
    "            print(\"\\nğŸ‘‰ í›„ë³´ ì²­í¬:\")\n",
    "            for i, (c, s) in enumerate(ranked, 1):\n",
    "                print(f\"{i}. {c} (score={s:.3f})\")\n",
    "            sel = input(\"ì„ íƒ ë²ˆí˜¸ (ì—”í„°=ì™„ì„±): \").strip()\n",
    "            if sel == \"\": break\n",
    "            try:\n",
    "                idx = int(sel)\n",
    "                if not (1 <= idx <= len(ranked)):\n",
    "                    print(\"âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"); continue\n",
    "            except Exception:\n",
    "                print(\"âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"); continue\n",
    "            chosen = ranked[idx-1][0]\n",
    "            context_tokens.append(chosen)\n",
    "            print(f\"âœ… ì„ íƒë¨: {chosen}\")\n",
    "            print(\"ğŸ§± í˜„ì¬ê¹Œì§€:\", \" \".join(context_tokens))\n",
    "            steps += 1\n",
    "            if steps >= 50:\n",
    "                print(\"â„¹ï¸ ìµœëŒ€ 50ê°œ ì²­í¬ê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìë™ ì™„ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                break\n",
    "        print(\"\\nğŸ—£ï¸ ìµœì¢… ë¬¸ì¥:\", \" \".join(context_tokens))\n",
    "        return \" \".join(context_tokens)\n",
    "\n",
    "# -------- í¸ì˜ í•¨ìˆ˜ --------\n",
    "def simulate_app_input():\n",
    "    picker = StepwiseChunkPicker(DATA_PATH_DEFAULT)\n",
    "    return picker.simulate_app_input(topk=TOPK_DEFAULT)\n",
    "\n",
    "def suggest_next(user_asr: str, selected_chunks):\n",
    "    picker = StepwiseChunkPicker(DATA_PATH_DEFAULT)\n",
    "    ctx = split_list_chunks_to_tokens(selected_chunks)\n",
    "    return picker.suggest(user_asr, ctx, topk=TOPK_DEFAULT)\n",
    "\n",
    "print(\"âœ… ì¤€ë¹„ ì™„ë£Œ!  data:\", DATA_PATH_DEFAULT)\n",
    "print(\"ì˜ˆ) simulate_app_input()  ë˜ëŠ”  suggest_next('ì£¼ë¬¸ ë„ì™€ë“œë¦´ê²Œìš”', ['ì•„ì´ìŠ¤','ëª¨ì¹´'])\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
