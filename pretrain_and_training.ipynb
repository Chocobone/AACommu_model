{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7af996",
   "metadata": {},
   "source": [
    "## ê¸°ì¡´ ë°ì´í„°ì…‹ì„ í•™ìŠµì— ì ì ˆí•˜ê²Œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1d4ad",
   "metadata": {},
   "source": [
    "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ê¸°ë³¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33553ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ì„¤ì •ê°’ (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ íŒŒì¼ëª…ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤) ---\n",
    "INPUT_ZIP_FILENAMES = [\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_01.ì…ì¥_ë°_ì´ìš©ì•ˆë‚´.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_02.ìë¦¬ì•ˆë‚´.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_03.ë©”ë‰´ì¶”ì²œ.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_04.ë©”ë‰´ì£¼ë¬¸.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_05.ì‹ìŒë£Œì„œë¹™.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_06.ê²°ì œ_ë°_í• ì¸_í¬ì¸íŠ¸ì ë¦½_ì•ˆë‚´.zip\",\n",
    "]\n",
    "INPUT_DIR = Path(\"./\") # ZIP íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬\n",
    "DEFAULT_CONTEXT = \"ì¹´í˜\"\n",
    "\n",
    "# ëª¨ë“  ëŒ€í™” ìŒì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬ì— ìœ ì§€)\n",
    "all_dialogue_pairs = []\n",
    "\n",
    "print(f\"ì´ {len(INPUT_ZIP_FILENAMES)}ê°œì˜ ì••ì¶• íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63563f",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì¶”ì¶œ ë° ê°€ê³µ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue_pairs(dialogue_data, context):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ëŒ€í™” JSON ë°ì´í„°(dialogue ë¦¬ìŠ¤íŠ¸)ì—ì„œ ì—°ì†ì ì¸ ë°œí™” ìŒ(Q&A)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    (Turn Nì˜ ë°œí™”ê°€ input_text, Turn N+1ì˜ ë°œí™”ê°€ output_textê°€ ë©ë‹ˆë‹¤.)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    if not isinstance(dialogue_data, list) or len(dialogue_data) < 2:\n",
    "        return pairs\n",
    "\n",
    "    for i in range(len(dialogue_data) - 1):\n",
    "        input_turn = dialogue_data[i]\n",
    "        output_turn = dialogue_data[i+1]\n",
    "        \n",
    "        input_text = input_turn.get('utterance', '').strip()\n",
    "        output_text = output_turn.get('utterance', '').strip()\n",
    "        \n",
    "        if input_text and output_text:\n",
    "            pairs.append({\n",
    "                \"context\": context,\n",
    "                \"input_text\": input_text,\n",
    "                # ì°¸ê³ : train_data_cafe_500.jsonì˜ ì¼ë°˜í™”/ì²­í¬í™” ë¡œì§ì€ ì œì™¸í•˜ê³  \n",
    "                #      ì›ë³¸ ë°œí™”ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "                \"output_text\": output_text \n",
    "            })\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8442c1d",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CafeModelTrainer:\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë°›ì•„ ëª¨ë¸ í•™ìŠµì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    ì‹¤ì œ í•™ìŠµ ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: í† í¬ë‚˜ì´ì €, ë°ì´í„° ë¡œë”, ëª¨ë¸ ì •ì˜).\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"T5_Cafe_Model\"):\n",
    "        self.model_name = model_name\n",
    "        self.data_size = 0\n",
    "        print(f\"ğŸ§  ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”: {self.model_name}\")\n",
    "\n",
    "    def preprocess_data(self, raw_data_list: list) -> list:\n",
    "        \"\"\"\n",
    "        raw_data_listë¥¼ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "        (ì˜ˆ: ë¬¸ì¥ í† í°í™”, ì²­í¬ ë¶„í• , ID ë§¤í•‘ ë“±)\n",
    "        \"\"\"\n",
    "        print(f\"âš™ï¸ ì´ {len(raw_data_list)}ê°œì˜ Q&A ìŒì„ ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # --- (ì‹¤ì œ êµ¬í˜„ í•„ìš” ë¶€ë¶„) ---\n",
    "        # dataset_pretraining.ipynbì—ì„œ ì œì‹œëœ í† í°í™” ë° ì²­í¬í™” ë¡œì§ì„ ì—¬ê¸°ì— ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        # ì˜ˆ: input_text, output_textë¥¼ ë¶„í•´í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ì²­í¬ í•™ìŠµ ìƒ˜í”Œ ìƒì„±\n",
    "        # í˜„ì¬ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        processed_data = [{\"features\": f\"Input: {d['input_text']}\", \"labels\": f\"Output: {d['output_text']}\"} \n",
    "                          for d in raw_data_list]\n",
    "        self.data_size = len(processed_data)\n",
    "        print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ. ì´ {self.data_size}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ.\")\n",
    "        return processed_data\n",
    "\n",
    "    def train(self, raw_data_list: list):\n",
    "        \"\"\"\n",
    "        ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        if not raw_data_list:\n",
    "            print(\"âŒ í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "\n",
    "        # 1. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        processed_data = self.preprocess_data(raw_data_list)\n",
    "\n",
    "        # 2. ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ (ì‹œë®¬ë ˆì´ì…˜)\n",
    "        print(\"ğŸš€ ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ ì‹œì‘...\")\n",
    "        \n",
    "        # --- (ì‹¤ì œ êµ¬í˜„ í•„ìš” ë¶€ë¶„) ---\n",
    "        # torch.load(model_path) ë˜ëŠ” tf.keras.Model(...) ë“±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ \n",
    "        # data_loader, optimizerë¥¼ ì„¤ì •í•œ í›„ model.fit(...)ì„ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        print(f\"ğŸ‰ ëª¨ë¸ '{self.model_name}' í•™ìŠµ ì™„ë£Œ! (í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {self.data_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65390a",
   "metadata": {},
   "source": [
    "main ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38127bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë°ì´í„° ì¶”ì¶œ\n",
    "for zip_filename in INPUT_ZIP_FILENAMES:\n",
    "    zip_path = INPUT_DIR / zip_filename\n",
    "    print(f\"\\n--- {zip_filename} ì²˜ë¦¬ ì‹œì‘ ---\")\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(f\"ğŸš¨ ê²½ê³ : {zip_filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤í‚µ)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            json_files = [f for f in zf.namelist() if f.endswith('.json')]\n",
    "            \n",
    "            for json_file in json_files:\n",
    "                try:\n",
    "                    with zf.open(json_file, 'r') as f:\n",
    "                        raw_data = f.read().decode('utf-8')\n",
    "                        data = json.loads(raw_data)\n",
    "                    \n",
    "                    dialogue_list = data.get('dialogue', [])\n",
    "                    pairs = extract_dialogue_pairs(dialogue_list, DEFAULT_CONTEXT)\n",
    "                    # íŒŒì¼ì— ì €ì¥í•˜ì§€ ì•Šê³  ë°”ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                    all_dialogue_pairs.extend(pairs)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    pass # ì˜¤ë¥˜ íŒŒì¼ì€ ë¬´ì‹œ\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {zip_filename}ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ìœ íš¨í•œ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ. ì´ {len(all_dialogue_pairs)}ê°œì˜ ì›ë³¸ Q&A ìŒì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2. ëª¨ë¸ í•™ìŠµ ì—°ë™ (íŒŒì¼ ì €ì¥ ì—†ì´ ë°”ë¡œ ì§„í–‰)\n",
    "trainer = CafeModelTrainer()\n",
    "trainer.train(all_dialogue_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fdde0",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random # ì„±ëŠ¥ ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from typing import List, Dict\n",
    "\n",
    "# --- ì„¤ì •ê°’ (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ íŒŒì¼ëª…ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤) ---\n",
    "VALIDATION_ZIP_FILENAMES = [\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_01.ì…ì¥_ë°_ì´ìš©ì•ˆë‚´_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_02.ìë¦¬ì•ˆë‚´_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_03.ë©”ë‰´ì¶”ì²œ_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_04.ë©”ë‰´ì£¼ë¬¸_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_05.ì‹ìŒë£Œì„œë¹™_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_06.ê²°ì œ_ë°_í• ì¸_í¬ì¸íŠ¸ì ë¦½_ì•ˆë‚´_01.json.zip\",\n",
    "]\n",
    "INPUT_DIR = Path(\"./\") # ZIP íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬\n",
    "DEFAULT_CONTEXT = \"ì¹´í˜\"\n",
    "\n",
    "# ëª¨ë“  ê²€ì¦ ìŒì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "all_validation_pairs = []\n",
    "\n",
    "print(f\"ì´ {len(VALIDATION_ZIP_FILENAMES)}ê°œì˜ ê²€ì¦ ì••ì¶• íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00a6ed",
   "metadata": {},
   "source": [
    "model validation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e38644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CafeModelValidator:\n",
    "    \"\"\"\n",
    "    í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ê²€ì¦ ë°ì´í„°ë¥¼ ì´ìš©í•´ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str = \"path/to/trained_cafe_model\"):\n",
    "        self.model_path = model_path\n",
    "        self._load_model()\n",
    "        print(f\"âœ… ëª¨ë¸ ê²€ì¦ê¸° ì´ˆê¸°í™” ì™„ë£Œ.\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤. \n",
    "        ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” torch.load(), tf.keras.models.load_model() ë“±ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ§  {self.model_path}ì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        # self.model = ActualModelClass.load(self.model_path)\n",
    "        self.model_loaded = True \n",
    "        \n",
    "    def _simulate_prediction(self, input_text: str) -> str:\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ì´ input_textì— ëŒ€í•´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        # ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ : model.generate(input_token_ids)\n",
    "        \n",
    "        # ì‹œë®¬ë ˆì´ì…˜: 80% í™•ë¥ ë¡œ ì •ë‹µê³¼ ìœ ì‚¬í•œ ì‘ë‹µ, 20% í™•ë¥ ë¡œ ë‹¤ë¥¸ ì‘ë‹µ ìƒì„±\n",
    "        if random.random() < 0.8:\n",
    "            return f\"ëª¨ë¸ ì‘ë‹µ: {input_text}ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µ\"\n",
    "        else:\n",
    "            return \"ëª¨ë¸ ì‘ë‹µ: ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?\"\n",
    "\n",
    "    def calculate_metrics(self, data: List[Dict]):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ì˜ ì‘ë‹µê³¼ ì •ë‹µ(Ground Truth)ì„ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        print(f\"ğŸ“Š ì´ {len(data)}ê°œì˜ ê²€ì¦ ìƒ˜í”Œë¡œ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘...\")\n",
    "        \n",
    "        for item in data:\n",
    "            input_text = item['input_text']\n",
    "            # ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ\n",
    "            predicted_output = self._simulate_prediction(input_text)\n",
    "            # ì •ë‹µ\n",
    "            ground_truth = item['output_text']\n",
    "            \n",
    "            predictions.append(predicted_output)\n",
    "            references.append(ground_truth)\n",
    "            \n",
    "            # ì‹¤ì œ ëª¨ë¸ í•™ìŠµì—ì„œëŠ” í† í° ID ë ˆë²¨ì—ì„œ Loss, Perplexity ê³„ì‚°\n",
    "\n",
    "        # --- (ì‹¤ì œ êµ¬í˜„ í•„ìš” ë¶€ë¶„) ---\n",
    "        # 1. HuggingFace datasetsë‚˜ torchmetrics ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        #    - BLEU, ROUGE, Exact Match, Perplexity ë“±\n",
    "        \n",
    "        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼\n",
    "        simulated_loss = random.uniform(0.5, 1.5)\n",
    "        simulated_accuracy = random.uniform(0.75, 0.95)\n",
    "        simulated_bleu = random.uniform(0.60, 0.85)\n",
    "        \n",
    "        print(\"\\n=== ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼ ===\")\n",
    "        print(f\"  - ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(data)}\")\n",
    "        print(f\"  - í‰ê·  ê²€ì¦ ì†ì‹¤ (Loss): {simulated_loss:.4f} (Simulated)\")\n",
    "        print(f\"  - ë°œí™” ì¼ì¹˜ ì •í™•ë„ (Accuracy): {simulated_accuracy:.2%} (Simulated)\")\n",
    "        print(f\"  - BLEU ì ìˆ˜ (ìì—°ì–´ ìƒì„± í’ˆì§ˆ): {simulated_bleu:.4f} (Simulated)\")\n",
    "        print(\"============================\\n\")\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bd407",
   "metadata": {},
   "source": [
    "ê²€ì¦ ë°ì´í„° ì¶”ì¶œ ë° í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e714df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ê²€ì¦ ë°ì´í„° ì¶”ì¶œ\n",
    "print(\"=== 1. ê²€ì¦ ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ===\")\n",
    "for zip_filename in VALIDATION_ZIP_FILENAMES:\n",
    "    zip_path = INPUT_DIR / zip_filename\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(f\"ğŸš¨ ê²½ê³ : {zip_filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤í‚µ)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            json_files = [f for f in zf.namelist() if f.endswith('.json')]\n",
    "            \n",
    "            for json_file in json_files:\n",
    "                try:\n",
    "                    with zf.open(json_file, 'r') as f:\n",
    "                        raw_data = f.read().decode('utf-8')\n",
    "                        data = json.loads(raw_data)\n",
    "                    \n",
    "                    dialogue_list = data.get('dialogue', [])\n",
    "                    pairs = extract_dialogue_pairs(dialogue_list, DEFAULT_CONTEXT)\n",
    "                    all_validation_pairs.extend(pairs)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ ({zip_filename}): {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ. ì´ {len(all_validation_pairs)}ê°œì˜ ê²€ì¦ ìŒì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 2. ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦\n",
    "if all_validation_pairs:\n",
    "    validator = CafeModelValidator()\n",
    "    validator.calculate_metrics(all_validation_pairs)\n",
    "else:\n",
    "    print(\"âŒ ê²€ì¦ ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ì„±ëŠ¥ ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
