{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4513a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ê¸°ë³¸ ì„¤ì • ë° ì¥ì¹˜ í™•ì¸ (AAC_model.ipynb) ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "MODEL_NAME = \"klue/bert-base\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ ---\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
    "INPUT_ZIP_FILENAMES = [\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_01.ì…ì¥_ë°_ì´ìš©ì•ˆë‚´.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_02.ìë¦¬ì•ˆë‚´.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_03.ë©”ë‰´ì¶”ì²œ.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_04.ë©”ë‰´ì£¼ë¬¸.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_05.ì‹ìŒë£Œì„œë¹™.zip\",\n",
    "    \"TL_01.ì‹ë‹¹ì¹´í˜_06.ê²°ì œ_ë°_í• ì¸_í¬ì¸íŠ¸ì ë¦½_ì•ˆë‚´.zip\",\n",
    "]\n",
    "# ZIP íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ (AAC_model.ipynbì˜ ë”ë¯¸ ê²½ë¡œ ëŒ€ì‹  ì‹¤ì œ ZIP ê²½ë¡œ ì‚¬ìš©)\n",
    "INPUT_DIR = Path(\"/local_datasets/AACommu/Training/02.ë¼ë²¨ë§ë°ì´í„°\") \n",
    "DEFAULT_CONTEXT = \"ì¹´í˜\"\n",
    "all_dialogue_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue_pairs_from_json(json_data, context):\n",
    "    \"\"\"\n",
    "    ì²¨ë¶€ëœ JSON íŒŒì¼ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ 'human_event.utterances.utterance_cap'ì„\n",
    "    input_textë¡œ, 'robot_response.answer'ë¥¼ output_textë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    video_data = json_data.get('video')\n",
    "    if not video_data:\n",
    "        return pairs\n",
    "\n",
    "    interactions = video_data.get('interactions', [])\n",
    "    \n",
    "    for interaction in interactions:\n",
    "        human_utterances = interaction.get('human_event', {}).get('utterances', [])\n",
    "        robot_responses = interaction.get('robot_response', [])\n",
    "        \n",
    "        # Human Input ì¶”ì¶œ\n",
    "        input_text = \"\"\n",
    "        if human_utterances:\n",
    "            # ì²« ë²ˆì§¸ ë°œí™”ì˜ 'utterance_cap'ì„ inputìœ¼ë¡œ ì‚¬ìš©\n",
    "            input_text = human_utterances[0].get('utterance_cap', '').strip()\n",
    "\n",
    "        # Robot Output ì¶”ì¶œ\n",
    "        output_text = \"\"\n",
    "        if robot_responses:\n",
    "            # ì²« ë²ˆì§¸ ë¡œë´‡ ì‘ë‹µì˜ 'answer'ë¥¼ outputìœ¼ë¡œ ì‚¬ìš©\n",
    "            output_text = robot_responses[0].get('answer', '').strip()\n",
    "        \n",
    "        if input_text and output_text:\n",
    "            pairs.append({\n",
    "                \"context\": context,\n",
    "                \"input_text\": input_text,\n",
    "                \"output_text\": output_text \n",
    "            })\n",
    "            \n",
    "    return pairs\n",
    "\n",
    "def load_and_extract_data():\n",
    "    \"\"\"ZIP íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ all_dialogue_pairsì— ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"ì´ {len(INPUT_ZIP_FILENAMES)}ê°œì˜ ì••ì¶• íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    for zip_filename in INPUT_ZIP_FILENAMES:\n",
    "        zip_path = INPUT_DIR / zip_filename\n",
    "        # print(f\"\\n--- {zip_filename} ì²˜ë¦¬ ì‹œì‘ ---\")\n",
    "        \n",
    "        if not zip_path.exists():\n",
    "            # print(f\"ğŸš¨ ê²½ê³ : {zip_filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤í‚µ)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                json_files = [f for f in zf.namelist() if f.endswith('.json')]\n",
    "                \n",
    "                for json_file in json_files:\n",
    "                    try:\n",
    "                        with zf.open(json_file, 'r') as f:\n",
    "                            raw_data = f.read().decode('utf-8')\n",
    "                            data = json.loads(raw_data)\n",
    "                        \n",
    "                        # ìˆ˜ì •ëœ ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©\n",
    "                        pairs = extract_dialogue_pairs_from_json(data, DEFAULT_CONTEXT)\n",
    "                        global all_dialogue_pairs\n",
    "                        all_dialogue_pairs.extend(pairs)\n",
    "                        \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            # print(f\"ğŸš¨ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nâœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ. ì´ {len(all_dialogue_pairs)}ê°œì˜ Q&A ìŒì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì¶”ì¶œëœ Q&A ìŒì„ BERT ë¶„ë¥˜ ëª¨ë¸ì— ë§ê²Œ ë³€í™˜ (AAC_model.ipynbì˜ load_and_preprocess_data ë¡œì§ì„ ëŒ€ì²´)\n",
    "    if not all_dialogue_pairs:\n",
    "        # ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë”ë¯¸ ë°ì´í„° ìƒì„±\n",
    "        print(\"âš ï¸ Warning: No data extracted. Using dummy data for structure.\")\n",
    "        df_combined = pd.DataFrame({\n",
    "            'text': [\"JSON íŒŒì¼ í•˜ë‚˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\", \"ì˜¤ëŠ˜ì˜ í•™ìŠµ ì£¼ì œëŠ” BERTì…ë‹ˆë‹¤.\", \"ë°ì´í„°ì…‹ì„ ë‹¨ì¼ íŒŒì¼ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\"], \n",
    "            'label': [0, 1, 0]\n",
    "        })\n",
    "    else:\n",
    "        # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        df_combined = pd.DataFrame(all_dialogue_pairs)\n",
    "        \n",
    "        # AAC_model.ipynb ë¡œì§ì— ë”°ë¼ text ì»¬ëŸ¼ê³¼ ì„ì˜ì˜ ì´ì§„ label ì»¬ëŸ¼ ìƒì„±\n",
    "        df_combined['text'] = df_combined['input_text'] # input_textë¥¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
    "        df_combined['label'] = 0\n",
    "        df_combined.loc[df_combined.index % 2 == 0, 'label'] = 1 # ì§ìˆ˜ ì¸ë±ìŠ¤ì— ì„ì˜ë¡œ ë ˆì´ë¸” 1 ë¶€ì—¬\n",
    "\n",
    "        # í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ë‚¨ê¹€\n",
    "        df_combined = df_combined.loc[:, ['text', 'label']]\n",
    "        \n",
    "    print(f\"Total data size (BERT Classification): {len(df_combined)}\")\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dafa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "df = load_and_extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3299bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Dataset ë° DataLoader ìƒì„± (AAC_model.ipynb) ---\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (stratify ì˜µì…˜ ì‚¬ìš©)\n",
    "if len(df) > 1:\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
    "else: # ë°ì´í„°ê°€ ë¶€ì¡±í•  ê²½ìš° ë¶„í•  ì—†ì´ ì‚¬ìš©\n",
    "    df_train = df\n",
    "    df_val = df.iloc[:1] if len(df) > 0 else pd.DataFrame({'text': [], 'label': []})\n",
    "    print(\"âš ï¸ Warning: Data size is too small for splitting (only 1 or 0 sample).\")\n",
    "\n",
    "# Dataset ë° DataLoader ìƒì„±\n",
    "train_dataset = TextDataset(\n",
    "    texts=df_train.text.to_list(),\n",
    "    labels=df_train.label.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "val_dataset = TextDataset(\n",
    "    texts=df_val.text.to_list(),\n",
    "    labels=df_val.label.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d27ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ëª¨ë¸ ì •ì˜ ë° ì´ˆê¸°í™” ---\n",
    "\n",
    "class AACommuModel(nn.Module):\n",
    "    def __init__(self, n_classes, model_name):\n",
    "        super(AACommuModel, self).__init__()\n",
    "        # ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ ë¡œë“œ\n",
    "        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        # ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¶œë ¥ ë ˆì´ì–´\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT ëª¨ë¸ì— ì…ë ¥ (pooled_outputë§Œ ì‚¬ìš©)\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™” (ë ˆì´ë¸” ìˆ˜ì— ë”°ë¼ í´ë˜ìŠ¤ ê°œìˆ˜ ì„¤ì •)\n",
    "N_CLASSES = df['label'].nunique() if len(df) > 0 else 2\n",
    "model = AACommuModel(N_CLASSES, MODEL_NAME)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d56d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ---\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for step, d in enumerate(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(f\"  Step {step}: Loss {loss.item():.4f}\")\n",
    "\n",
    "    return correct_predictions.double() / n_examples, sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. í•™ìŠµ ë£¨í”„ ì‹¤í–‰ ---\n",
    "\n",
    "if len(df_train) > 0 and len(df_val) > 0:\n",
    "    # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # í•™ìŠµ ì‹œì‘\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_accuracy = 0\n",
    "\n",
    "    print(\"\\n=== Start Model Training ===\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\n[Epoch {epoch + 1}/{EPOCHS}]')\n",
    "        \n",
    "        # í›ˆë ¨ (Train)\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model,\n",
    "            train_data_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            len(df_train)\n",
    "        )\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        \n",
    "        # ê²€ì¦ (Validation)\n",
    "        val_acc, val_loss = eval_model(\n",
    "            model,\n",
    "            val_data_loader,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            len(df_val)\n",
    "        )\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥ ë° ìµœì  ëª¨ë¸ ì €ì¥\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        \n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), 'AACommu_model_best_weights.pt')\n",
    "            best_accuracy = val_acc\n",
    "            print(\"Model weights saved: AACommu_model_best_weights.pt\")\n",
    "\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "else:\n",
    "    print(\"\\nâŒ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ZIP íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdef55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation datasetsì„ ì‚¬ìš©í•œ ëª¨ë¸ ê²€ì¦ ---\n",
    "VALIDATION_ZIP_FILENAMES = [\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_01.ì…ì¥_ë°_ì´ìš©ì•ˆë‚´_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_02.ìë¦¬ì•ˆë‚´_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_03.ë©”ë‰´ì¶”ì²œ_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_04.ë©”ë‰´ì£¼ë¬¸_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_05.ì‹ìŒë£Œì„œë¹™_01.json.zip\",\n",
    "    \"VL_01.ì‹ë‹¹ì¹´í˜_06.ê²°ì œ_ë°_í• ì¸_í¬ì¸íŠ¸ì ë¦½_ì•ˆë‚´_01.json.zip\",\n",
    "]\n",
    "INPUT_DIR = Path(\"/local_datasets/AACommu/Validation/02.ë¼ë²¨ë§ë°ì´í„°\") # ZIP íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬\n",
    "DEFAULT_CONTEXT = \"ì¹´í˜\"\n",
    "\n",
    "# ëª¨ë“  ê²€ì¦ ìŒì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "all_validation_pairs = []\n",
    "\n",
    "print(f\"ì´ {len(VALIDATION_ZIP_FILENAMES)}ê°œì˜ ê²€ì¦ ì••ì¶• íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
