{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed88f6c",
   "metadata": {},
   "source": [
    "# ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- ê¸°ë³¸ ì„¤ì • ---\n",
    "INPUT_PATH = \"/content/KakaoData.csv\"\n",
    "OUT_DIR = Path(\"/content/kakao_clean_final3\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- ì•ˆì „í•˜ê²Œ CSV ì½ê¸° ---\n",
    "def safe_read_csv(path):\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(\"CSV ì¸ì½”ë”© ë¬¸ì œ ë°œìƒ\")\n",
    "\n",
    "df = safe_read_csv(INPUT_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df = df[[\"req\", \"res\"]].dropna()\n",
    "\n",
    "# --- ì •ê·œì‹ íŒ¨í„´ ì •ì˜ ---\n",
    "# í•œê¸€ ìëª¨ (ã…‹,ã…,ã… ,ã…œ ë“±) + ë¶„í•´ ìëª¨ (á„, á…² ë“±)\n",
    "EMOTE_CLASS = r\"[\\u314B\\u314E\\u3161\\u315C\\u110F\\u1112\\u1172\\u116E]\"\n",
    "# ê¸°í˜¸ë¥˜: ^^, ã…¡ã…¡, :, ;, ~, -, _, = ë“±\n",
    "SYMBOL_CLASS = r\"[\\^;:~\\-_=]+|[ã…¡]+\"\n",
    "# ì˜ë¯¸ ì—†ëŠ” ê°íƒ„ì‚¬ (ë¬¸ì¥ ë§¨ì• ë˜ëŠ” ì „ì²´ ë‹¨ì–´ë¡œ ì¡´ì¬)\n",
    "FILLER_WORDS = [\n",
    "    # ë‹¨ìˆœ ê°íƒ„ì‚¬\n",
    "    \"ìŒ\", \"ì‘\", \"ì•„\", \"ì–´\", \"ì˜¤\", \"í \", \"ì•—\", \"ì›…\", \"ì•„í•˜\", \"ì˜¤í˜¸\", \"ì—íœ´\", \"ì—¥\", \"ê·¸ëƒ¥\", \"ì•„ë‹ˆ\", \"í‰\", \"íœ´\", \"í—¤í—·\", \"ì•¼ì“°\", \"ê·¸\",\n",
    "    \"ì™€\", \"í—‰\"\n",
    "    # ë¬¼ìŒí‘œ ë¶™ì€ ê°íƒ„ì‚¬\n",
    "    \"ìŒ?\", \"ì‘?\", \"ì•„?\", \"ì–´?\", \"ì˜¤?\", \"í ?\", \"ì›…?\", \"ì•—?\", \"ì•„í•˜?\", \"ì˜¤í˜¸?\", \"ì—¥?\", \"ì—¥...?\",\n",
    "    # ì (.) ë¶™ì€ ê°íƒ„ì‚¬ (ì•„.., ìŒ..., ì‘.... ë“±)\n",
    "    \"ìŒ.\", \"ì‘.\", \"ì•„.\", \"ì–´.\", \"ì˜¤.\", \"í .\", \"ì›….\", \"ì•—.\", \"ì•„í•˜.\", \"ì˜¤í˜¸.\", \"ì—¥.\",\n",
    "    \"ìŒ..\", \"ì‘..\", \"ì•„..\", \"ì–´..\", \"ì˜¤..\", \"í ..\", \"ì›…..\", \"ì•—..\", \"ì•„í•˜..\", \"ì˜¤í˜¸..\", \"ì—¥..\",\n",
    "    \"ìŒ...\", \"ì‘...\", \"ì•„...\", \"ì–´...\", \"ì˜¤...\", \"í ...\", \"ì›…...\", \"ì•—...\", \"ì•„í•˜...\", \"ì˜¤í˜¸...\", \"ì—¥...\", \"ì™€...\",\n",
    "]\n",
    "\n",
    "# ê°íƒ„ì‚¬ íŒ¨í„´ (ë¬¸ì¥ ì•ë’¤ ê³µë°± í¬í•¨ ë˜ëŠ” ë¬¸ì¥ ì „ì²´ì¼ ë•Œ)\n",
    "# ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜ (ë‹¨ì–´ ê²½ê³„ ê¸°ì¤€)\n",
    "FILLER_RE = re.compile(\n",
    "    rf\"(^|\\s)({'|'.join(map(re.escape, FILLER_WORDS))})(\\s|$)\"\n",
    ")\n",
    "\n",
    "# ë¯¸ë¦¬ ì»´íŒŒì¼í•œ ì •ê·œì‹\n",
    "RE_EMOTE = re.compile(rf\"(?:{EMOTE_CLASS}|ã…‹|ã…|ã… |ã…œ)+\")\n",
    "RE_SYMBOL = re.compile(rf\"(?:{SYMBOL_CLASS})+\")\n",
    "RE_URL = re.compile(r\"https?://\\S+\")\n",
    "RE_EMAIL = re.compile(r\"\\S+@\\S+\\.\\S+\")\n",
    "\n",
    "# --- ì •ê·œí™” ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---\n",
    "def normalize_and_clean(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    # 1ï¸âƒ£ ìœ ë‹ˆì½”ë“œ ì •ê·œí™” ë° ì œë¡œí­ ë¬¸ì ì œê±°\n",
    "    s = ud.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", s)\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable())\n",
    "    s = s.strip()\n",
    "\n",
    "    # 2ï¸âƒ£ ê°ì •/ê¸°í˜¸ ì œê±°\n",
    "    s = RE_EMOTE.sub(\"\", s)     # ã…‹ã…‹, ã…ã…, ã… ã… , á„á„, á…² ë“±\n",
    "    s = RE_SYMBOL.sub(\"\", s)    # ^^, ã…¡ã…¡, :, ; ë“±\n",
    "    # 3ï¸âƒ£ ì˜ë¯¸ ì—†ëŠ” ê°íƒ„ì‚¬ ì œê±°\n",
    "    s = FILLER_RE.sub(\" \", s)\n",
    "    # 4ï¸âƒ£ URL/ì´ë©”ì¼ ì œê±°\n",
    "    s = RE_URL.sub(\"\", s)\n",
    "    s = RE_EMAIL.sub(\"\", s)\n",
    "    # 5ï¸âƒ£ ê³µë°± ì •ë¦¬\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# --- ì „ì²˜ë¦¬ ì ìš© ---\n",
    "df[\"req_clean\"] = df[\"req\"].apply(normalize_and_clean)\n",
    "df[\"res_clean\"] = df[\"res\"].apply(normalize_and_clean)\n",
    "\n",
    "# --- ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ ì œê±° ---\n",
    "df = df[df[\"req_clean\"].str.len() > 1]\n",
    "df = df[df[\"res_clean\"].str.len() > 1]\n",
    "df = df[df[\"req_clean\"].str.len() < 300]\n",
    "df = df[df[\"res_clean\"].str.len() < 300]\n",
    "\n",
    "# --- ì¤‘ë³µ/ì—ì½” ì œê±° ---\n",
    "df = df[df[\"req_clean\"] != df[\"res_clean\"]]\n",
    "df = df.drop_duplicates(subset=[\"req_clean\", \"res_clean\"]).reset_index(drop=True)\n",
    "\n",
    "# --- ê²°ê³¼ ì €ì¥ ---\n",
    "clean_csv = OUT_DIR / \"KakaoData_clean_final3.csv\"\n",
    "df[[\"req_clean\", \"res_clean\"]].rename(columns={\"req_clean\":\"req\",\"res_clean\":\"res\"}).to_csv(\n",
    "    clean_csv, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ê°ì •Â·ê¸°í˜¸Â·ê°íƒ„ì‚¬ ì œê±° í›„ {len(df)}ê°œ ëŒ€í™”ìŒ ì €ì¥ë¨\")\n",
    "print(f\"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {clean_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('/content/kakao_clean_final3/KakaoData_clean_final3.csv')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import unicodedata as ud\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "INPUT_CSV  = \"/content/cafe_qa_korean_polite_v2.csv\"\n",
    "OUT_JSONL  = \"/content/cafe_qa_korean_polite_v2.jsonl\"\n",
    "\n",
    "DEFAULT_CONTEXT = \"ì¹´í˜\"\n",
    "DEFAULT_SPEAKER = \"clerk\"\n",
    "\n",
    "REQ_COL = \"req\"\n",
    "RES_COL = \"res\"\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(\"CSV ì¸ì½”ë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ===== ìœ í‹¸: ê°„ë‹¨ ì •ê·œí™” & ê³µë°± ì •ë¦¬ =====\n",
    "def norm_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = ud.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ===== í† í° ë¶„í•  ì •ì±… =====\n",
    "def tokenize_korean_sentence(s: str):\n",
    "    s = norm_text(s)\n",
    "    if not s:\n",
    "        return []\n",
    "    # ê³µë°± ê¸°ì¤€ í† í°í™”\n",
    "    tokens = s.split(\" \")\n",
    "    # ë¹ˆ í† í° ì œê±°\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return tokens\n",
    "\n",
    "# ===== ë©”ì¸ ë³€í™˜ =====\n",
    "df = safe_read_csv(INPUT_CSV)\n",
    "# ì»¬ëŸ¼ ì´ë¦„ ê³µë°± ì œê±°\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸\n",
    "assert REQ_COL in df.columns and RES_COL in df.columns, f\"CSVì— '{REQ_COL}', '{RES_COL}' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. í˜„ì¬: {df.columns}\"\n",
    "\n",
    "n_rows = 0\n",
    "n_lines = 0\n",
    "\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for _, row in df.iterrows():\n",
    "        user_asr = norm_text(str(row[REQ_COL]))\n",
    "        target   = norm_text(str(row[RES_COL]))\n",
    "\n",
    "        # ëª©í‘œ ë¬¸ì¥ í† í°í™”\n",
    "        tokens = tokenize_korean_sentence(target)\n",
    "        if len(tokens) == 0 or len(user_asr) == 0:\n",
    "            continue\n",
    "\n",
    "        # ê·œì¹™\n",
    "        for i, tok in enumerate(tokens):\n",
    "            rec = {\n",
    "                \"context\": DEFAULT_CONTEXT,\n",
    "                \"speaker\": DEFAULT_SPEAKER,\n",
    "                \"user_asr\": user_asr,\n",
    "                \"selected_chunks\": tokens[:i],\n",
    "                \"target_chunk\": tok\n",
    "            }\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n_lines += 1\n",
    "\n",
    "        n_rows += 1\n",
    "\n",
    "print(f\"âœ… ë³€í™˜ ì™„ë£Œ! ì´ {n_rows}ê°œ ë¬¸ì¥ì—ì„œ {n_lines}ê°œ ë¼ì¸ ìƒì„±\")\n",
    "print(f\"ğŸ“„ ì¶œë ¥ íŒŒì¼: {OUT_JSONL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bebd1",
   "metadata": {},
   "source": [
    "# ì¹´í˜ ì¶”ê°€ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c031cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import unicodedata as ud\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "INPUT_CSV  = \"/content/cafe_qa_korean_polite_v2.csv\"   # ì—…ë¡œë“œí•œ CSV ê²½ë¡œ\n",
    "OUT_JSONL  = \"/content/cafe_qa_korean_polite_v2.jsonl\"           # ì¶œë ¥ JSONL ê²½ë¡œ\n",
    "\n",
    "DEFAULT_CONTEXT = \"\"          # ì˜ˆ: \"ì¹´í˜\" ê°™ì´ ë„£ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ì ê¸°\n",
    "DEFAULT_SPEAKER = \"clerk\"     # ì˜ˆ: \"clerk\", \"agent\", \"assistant\" ë“±\n",
    "\n",
    "# CSVì˜ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥´ë©´ ì—¬ê¸°ì„œ ë°”ê¿”ì¤˜:\n",
    "REQ_COL = \"req\"               # ì‚¬ìš©ì ë°œí™”(ì§ˆë¬¸/ìš”ì²­) â†’ user_asrë¡œ ë“¤ì–´ê°\n",
    "RES_COL = \"res\"               # ëª¨ë¸/ì ì›ì´ ë§í•  ëª©í‘œ ë¬¸ì¥ â†’ ì¡°ê°ìœ¼ë¡œ ë¶„ë¦¬í•  ëŒ€ìƒ\n",
    "\n",
    "# ===== ìœ í‹¸: ì•ˆì „í•œ CSV ì½ê¸° =====\n",
    "def safe_read_csv(path):\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(\"CSV ì¸ì½”ë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ===== ìœ í‹¸: ê°„ë‹¨ ì •ê·œí™” & ê³µë°± ì •ë¦¬ (ì„ íƒ) =====\n",
    "def norm_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = ud.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ===== í† í° ë¶„í•  ì •ì±… =====\n",
    "# ê¸°ë³¸ì€ ê³µë°± ê¸°ë°˜. ì´ë¯¸ ì „ì²˜ë¦¬ì—ì„œ ì´ëª¨/ê¸°í˜¸/ì¤‘ë³µ ê³µë°±ì„ ì •ë¦¬í–ˆìœ¼ë‹ˆ ì¼ë°˜ì ìœ¼ë¡œ ì¶©ë¶„í•¨.\n",
    "# í•„ìš”í•˜ë©´ ì•„ë˜ì—ì„œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €(MeCab ë“±)ë¡œ êµì²´ ê°€ëŠ¥.\n",
    "def tokenize_korean_sentence(s: str):\n",
    "    s = norm_text(s)\n",
    "    if not s:\n",
    "        return []\n",
    "    # ê³µë°± ê¸°ì¤€ í† í°í™”\n",
    "    tokens = s.split(\" \")\n",
    "    # ë¹ˆ í† í° ì œê±°\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return tokens\n",
    "\n",
    "# ===== ë©”ì¸ ë³€í™˜ =====\n",
    "df = safe_read_csv(INPUT_CSV)\n",
    "# ì»¬ëŸ¼ ì´ë¦„ ê³µë°± ì œê±°\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸\n",
    "assert REQ_COL in df.columns and RES_COL in df.columns, f\"CSVì— '{REQ_COL}', '{RES_COL}' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. í˜„ì¬: {df.columns}\"\n",
    "\n",
    "n_rows = 0\n",
    "n_lines = 0\n",
    "\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for _, row in df.iterrows():\n",
    "        user_asr = norm_text(str(row[REQ_COL]))\n",
    "        target   = norm_text(str(row[RES_COL]))\n",
    "\n",
    "        # ëª©í‘œ ë¬¸ì¥ í† í°í™”\n",
    "        tokens = tokenize_korean_sentence(target)\n",
    "        if len(tokens) == 0 or len(user_asr) == 0:\n",
    "            continue\n",
    "\n",
    "        # ì˜ˆì‹œì™€ ë™ì¼í•œ ê·œì¹™:\n",
    "        #  [\"ì•„ì´ìŠ¤\",\"ì•„ë©”ë¦¬ì¹´ë…¸\",\"í•œ\",\"ì”\",\"ì£¼ì„¸ìš”\"] -> 5ì¤„ ìƒì„±\n",
    "        #   selected_chunks=[],        target_chunk=\"ì•„ì´ìŠ¤\"\n",
    "        #   selected_chunks=[\"ì•„ì´ìŠ¤\"], target_chunk=\"ì•„ë©”ë¦¬ì¹´ë…¸\"\n",
    "        #   ...\n",
    "        for i, tok in enumerate(tokens):\n",
    "            rec = {\n",
    "                \"context\": DEFAULT_CONTEXT,\n",
    "                \"speaker\": DEFAULT_SPEAKER,\n",
    "                \"user_asr\": user_asr,\n",
    "                \"selected_chunks\": tokens[:i],\n",
    "                \"target_chunk\": tok\n",
    "            }\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            n_lines += 1\n",
    "\n",
    "        n_rows += 1\n",
    "\n",
    "print(f\"âœ… ë³€í™˜ ì™„ë£Œ! ì´ {n_rows}ê°œ ë¬¸ì¥ì—ì„œ {n_lines}ê°œ ë¼ì¸ ìƒì„±\")\n",
    "print(f\"ğŸ“„ ì¶œë ¥ íŒŒì¼: {OUT_JSONL}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
